# Project Plan - Critical Review & Issues
## Comprehensive Analysis of All Planning Documents

**Reviewer:** AI Senior Developer
**Date:** 2025-01-09
**Status:** CRITICAL ISSUES FOUND - REQUIRES FIXES

---

## üö® CRITICAL ISSUES (Must Fix Before Starting)

### Issue #1: WebSocket vs Polling Contradiction ‚ö†Ô∏è

**Problem:**
```
MVP-PLAN.md:                           "NO WebSocket (use polling)"
system-monitor-dashboard-project-plan: Sprint 3 - Full WebSocket implementation
RELEASE-PLAN.md Phase 1.4:            "Implement WebSocket Hub"
FUTURE-FEATURES.md:                    "WebSocket as enhancement"
```

**Impact:** AI won't know whether to build WebSocket or polling
**Severity:** HIGH - Blocks development

**Recommendation:**
```
DECISION NEEDED: Choose ONE approach for MVP

Option A: Polling Only (RECOMMENDED for MVP)
‚úÖ Simpler to implement
‚úÖ Easier to debug
‚úÖ 1-second latency is acceptable for demo
‚úÖ Can upgrade to WebSocket later
‚è±Ô∏è Implementation: 1 day

Option B: WebSocket from Start
‚ùå More complex
‚ùå More debugging needed
‚ùå Overkill for MVP
‚è±Ô∏è Implementation: 3-4 days

RECOMMENDATION: Use polling for MVP (v1.0), add WebSocket in v1.1 if needed
```

---

### Issue #2: Database Contradiction ‚ö†Ô∏è

**Problem:**
```
MVP-PLAN.md:                           "NO database (in-memory only)"
system-monitor-dashboard-project-plan: PostgreSQL in docker-compose
RELEASE-PLAN.md Phase 1.1:            "PostgreSQL service (for future use)"
VERSION-ROADMAP.md:                    "Database in v1.4"
```

**Impact:** Confusion about whether to set up database
**Severity:** MEDIUM - Wastes time if set up unnecessarily

**Recommendation:**
```
DECISION: Do NOT include PostgreSQL in MVP docker-compose

Reasoning:
- MVP stores metrics in-memory (60s history)
- No historical data needed for initial demo
- Reduces complexity
- Saves setup time

Action:
1. Remove PostgreSQL from Phase 1 docker-compose
2. Add it in v1.4 when historical data is needed
3. Update all documents to reflect this

Updated docker-compose.yml for MVP:
services:
  backend:
    # ...
  frontend:
    # ...
  # NO postgres in MVP!
```

---

### Issue #3: Timeline Inconsistency ‚ö†Ô∏è

**Problem:**
```
MVP-PLAN.md:                           "4 weeks"
system-monitor-dashboard-project-plan: "Sprint 1-9" = 9 weeks!
RELEASE-PLAN.md:                       "4-6 weeks"
VERSION-ROADMAP.md:                    "4 weeks"
```

**Impact:** Unclear how long MVP actually takes
**Severity:** HIGH - Sets wrong expectations

**Analysis:**
```
system-monitor-dashboard-project-plan.md includes:
- Sprint 1-6: Core MVP (6 weeks)
- Sprint 7-8: CI/CD + AWS (2 weeks) ‚Üê NOT MVP!
- Sprint 9: Polish (1 week)

This is NOT a 4-week MVP, it's a 6-week MVP + 3 weeks deployment
```

**Recommendation:**
```
REALISTIC TIMELINE for MVP:

Optimistic (4 weeks):
- Requires full-time work
- AI has no blockers
- No major debugging needed
- Previous Go/React experience

Realistic (5-6 weeks):
- Part-time work
- Normal debugging time
- Learning curve included
- Some blocked time

Conservative (8 weeks):
- First time with Go
- Part-time (10-15 hrs/week)
- Includes learning time

RECOMMENDATION: Plan for 5-6 weeks, aim for 4 weeks
Update all documents to say "4-6 weeks" consistently
```

---

### Issue #4: Pre-built Scenarios Confusion ‚ö†Ô∏è

**Problem:**
```
MVP-PLAN.md:                           "OUT OF SCOPE"
system-monitor-dashboard-project-plan: "Sprint 6 - Scenarios (in MVP)"
RELEASE-PLAN.md:                       "Phase 1.10 - Optional"
VERSION-ROADMAP.md:                    "v1.3 - After MVP"
```

**Impact:** Unclear if scenarios are in MVP or not
**Severity:** MEDIUM - Scope creep risk

**Recommendation:**
```
CLEAR DECISION: Scenarios are NOT in MVP

MVP Scope (v1.0):
‚úÖ 4 manual actions (CPU, Memory, Disk, Network)
‚ùå NO pre-built scenarios

Post-MVP (v1.3):
‚úÖ 3 pre-built scenarios
‚úÖ Timeline visualization
‚úÖ Auto-play mode

Why defer?
- MVP proves core concept (manual actions work)
- Scenarios add complexity (timeline execution, sequencing)
- Can validate MVP before building scenarios
- Saves 1-2 weeks for initial demo

Action: Update system-monitor-dashboard-project-plan.md to remove Sprint 6 from MVP
```

---

### Issue #5: Phase vs Version Numbering Mismatch ‚ö†Ô∏è

**Problem:**
```
RELEASE-PLAN.md:    Phase 1.1, 1.2, 1.3... 1.10
VERSION-ROADMAP.md: v1.0, v1.1, v1.2, v1.3

What is Phase 1.10? Is that v1.0 or something else?
```

**Impact:** Confusing to track progress
**Severity:** MEDIUM - Communication issues

**Recommendation:**
```
ALIGN NAMING:

RELEASE-PLAN.md should use:
- Phase 1: Local Development (contains multiple sub-phases)
  - Phase 1.A: Foundation
  - Phase 1.B: Metrics
  - Phase 1.C: Actions
  - Phase 1.D: Frontend
  - Phase 1.E: Polish
  - ‚úÖ Complete = v1.0 Release

- Phase 1.5: Showcase Deployment
  - ‚úÖ Complete = v1.2 Release

- Phase 2: AWS Dev
  - ‚úÖ Complete = v2.0 Release

VERSION-ROADMAP.md should map to phases:
- v1.0 = Phase 1 complete
- v1.1 = Polish only (no new phase)
- v1.2 = Phase 1.5 complete
- v2.0 = Phase 2 complete

Action: Simplify RELEASE-PLAN.md phase numbering
```

---

### Issue #6: Testing Coverage Target Inconsistency

**Problem:**
```
MVP-PLAN.md:                "70% coverage"
AI-DEVELOPMENT-RULES.md:    "MINIMUM 70%, TARGET 80%"
system-monitor-dashboard-project-plan: ">80% coverage"
```

**Impact:** Unclear quality bar
**Severity:** LOW - But needs clarity

**Recommendation:**
```
STANDARD COVERAGE TARGETS:

MVP (v1.0):
- Minimum: 70%
- Target: 75%
- Stretch: 80%

Post-MVP (v1.1+):
- Minimum: 75%
- Target: 80%
- Stretch: 85%

Critical code (safety limits, action cancellation):
- MUST be 100% covered

Action: Update all documents to use consistent targets
```

---

### Issue #7: Folder Structure Inconsistency

**Problem:**
```
MVP-PLAN.md:                           internal/actions/
system-monitor-dashboard-project-plan: internal/loadgen/
AI-DEVELOPMENT-RULES.md:               internal/actions/
```

**Impact:** AI won't know which folder to create
**Severity:** LOW - But causes confusion

**Recommendation:**
```
STANDARD FOLDER NAMING:

backend/
  internal/
    metrics/     ‚Üê Metrics collection
    actions/     ‚Üê Load generation (CPU, Memory, Disk, Network)
    api/         ‚Üê HTTP handlers
    websocket/   ‚Üê WebSocket (if implemented)

Reasoning:
- "actions" is clearer than "loadgen"
- Matches the domain language (user triggers "actions")
- Simpler naming

Action: Update all documents to use internal/actions/
```

---

## ‚ö†Ô∏è MODERATE ISSUES (Should Fix)

### Issue #8: Cost Estimates May Be Outdated

**Current Estimates:**
```
Railway.app: $5-10/month
Render.com: $7/month starter
AWS Dev: $75-100/month
AWS Prod: $150-200/month
```

**Concerns:**
- Railway.app pricing changed recently
- AWS costs can vary significantly
- No budget for domain name included

**Recommendation:**
```
UPDATED COST ESTIMATES (January 2025):

Showcase Hosting:
- Railway.app: $5/month (Hobby plan) OR $0 with trial credits
- Render.com: Free tier (with sleep) OR $7/month
- Fly.io: $5-10/month depending on usage
+ Domain (optional): $10-15/year

AWS Dev (minimal):
- ECS Fargate (2 tasks): $25-35/month
- ALB: $16/month
- RDS t3.micro: $15/month
- Data transfer: $5-10/month
- CloudWatch: $5/month
= Total: $66-81/month (lower than estimated)

AWS Prod (with redundancy):
- ECS Fargate (4+ tasks): $50-70/month
- ALB: $16/month
- RDS Multi-AZ: $30-40/month
- Data transfer: $10-20/month
- CloudWatch: $10/month
- Backups: $5/month
= Total: $121-161/month

Action: Verify current pricing before deployment
```

---

### Issue #9: Unrealistic "2-3 Days" for Showcase Deployment

**Claim:**
```
VERSION-ROADMAP.md: "Phase 1.5 - 2-3 days"
```

**Reality Check:**
```
Day 1: Platform setup + deployment (4-6 hours)
Day 2: Showcase features (rate limiting, demo mode) (6-8 hours)
Day 3: Testing + fixes (4-6 hours)
Day 4: Documentation + sharing (2-3 hours)

= 16-23 hours of actual work
= 2-3 days FULL-TIME or 4-5 days part-time
```

**Recommendation:**
```
REALISTIC ESTIMATE: 3-5 days

Full-time: 3 days
Part-time: 4-5 days
Includes: Learning platform, debugging, testing

Action: Update VERSION-ROADMAP.md to say "3-5 days"
```

---

### Issue #10: Missing Safety Limit Testing Strategy

**Problem:**
AI-DEVELOPMENT-RULES.md says safety limits are CRITICAL, but no specific test plan.

**Recommendation:**
```
MANDATORY SAFETY TESTS (Before MVP Complete):

Test 1: CPU Safety Limit
- Trigger CPU stress with target 95%
- ‚úÖ PASS: CPU stays ‚â§95%
- ‚ùå FAIL: CPU exceeds 95%

Test 2: Emergency Shutdown
- Trigger CPU stress
- Manually increase system load
- ‚úÖ PASS: Action stops when CPU hits 98%
- ‚ùå FAIL: System crashes

Test 3: Memory Safety Limit
- Trigger memory surge at 25% of RAM
- ‚úÖ PASS: Memory doesn't exceed 25%
- ‚ùå FAIL: Memory exceeds limit

Test 4: Action Cancellation
- Trigger action
- Stop it after 5 seconds
- ‚úÖ PASS: Stops within 1 second
- ‚ùå FAIL: Doesn't stop or takes >1s

Test 5: Concurrent Actions Safety
- Trigger all 4 actions simultaneously
- ‚úÖ PASS: System remains responsive
- ‚ùå FAIL: System freezes or crashes

Test 6: Resource Cleanup
- Trigger disk action
- Check temp files after completion
- ‚úÖ PASS: All temp files deleted
- ‚ùå FAIL: Temp files remain

Action: Add "Safety Test Suite" section to MVP-PLAN.md
```

---

## üí° OPTIMIZATION OPPORTUNITIES

### Optimization #1: Simplify Initial Project Structure

**Current Plan:**
15+ directories and files on Day 1

**Optimized Plan:**
```
Start minimal, add as needed:

Day 1:
monitoring-dashboard/
  backend/
    cmd/server/main.go
    go.mod
  frontend/
    (empty - start Day 5)
  README.md

Day 3: Add internal/metrics/
Day 5: Add internal/actions/
Day 7: Add frontend/
Day 10: Add tests/

Why?
- Reduces initial setup time
- Less overwhelming for AI
- Easier to debug small pieces
- Can refactor structure later if needed
```

---

### Optimization #2: Defer Non-Critical Features

**Current MVP Scope has:**
- Event log
- Active actions widget
- Multiple chart types (gauges + line charts)

**Optimization:**
```
MVP v1.0 (Core Demo):
‚úÖ 4 actions work
‚úÖ 4 metrics shown (simple numbers)
‚úÖ Basic line charts
‚ùå Event log (defer to v1.1)
‚ùå Gauges (defer to v1.1)
‚ùå Active actions widget (defer to v1.1)

Result: Save 3-4 days, get to "working demo" faster

Then add polish in v1.1:
- Event log
- Fancy gauges
- Active actions widget
- Animations
```

---

### Optimization #3: Start Without Docker, Add Later

**Current Plan:**
Set up Docker on Day 1-2

**Alternative:**
```
Week 1-2: Run backend/frontend locally
- go run cmd/server/main.go
- npm run dev
- Faster iteration
- Easier debugging

Week 3: Dockerize once it works
- Create Dockerfiles
- Test docker-compose
- Verify deployment works

Why?
- Docker adds complexity during development
- Hot reload is easier without Docker
- Can test Docker separately
- Still have Docker for demo in Week 4

Saves: 1-2 days during development
```

---

### Optimization #4: Use Feature Flags for Env-Specific Limits

**Problem:**
Different safety limits for local vs public

**Current Plan:**
Hardcode different limits in config

**Better:**
```go
// config/config.go
type Config struct {
    Environment string  // "local", "showcase", "production"
    SafetyLimits SafetyLimits
}

type SafetyLimits struct {
    MaxCPUPercent    int
    MaxMemoryPercent int
    MaxConcurrent    int
}

func LoadConfig() Config {
    env := os.Getenv("ENV")

    switch env {
    case "local":
        return Config{
            SafetyLimits: SafetyLimits{
                MaxCPUPercent: 95,
                MaxMemoryPercent: 25,
                MaxConcurrent: 5,
            },
        }
    case "showcase":
        return Config{
            SafetyLimits: SafetyLimits{
                MaxCPUPercent: 80,
                MaxMemoryPercent: 20,
                MaxConcurrent: 3,
            },
        }
    }
}

Benefits:
- Single codebase
- Easy to test different limits
- Clear environment distinction
- No code changes for deployment
```

---

### Optimization #5: Simplify Metric Collection

**Current Plan:**
4 metrics with complex per-core, per-partition breakdown

**Optimized MVP:**
```go
// Start simple
type Metrics struct {
    CPU     float64  // Total CPU %
    Memory  float64  // Memory %
    Disk    float64  // Disk I/O ops/sec
    Network float64  // Network MB/s
}

// Defer to v1.1
type DetailedMetrics struct {
    CPU struct {
        Total    float64
        PerCore  []float64     // ‚Üê Add later
        LoadAvg  [3]float64    // ‚Üê Add later
    }
    Memory struct {
        Percent  float64
        Total    uint64        // ‚Üê Add later
        Used     uint64        // ‚Üê Add later
        Free     uint64        // ‚Üê Add later
    }
}

Why?
- Proves concept faster (1-2 days vs 4-5 days)
- Simpler charts
- Can add detail later
- Still impressive demo
```

---

## üìã MISSING ELEMENTS

### Missing #1: Rollback Plan

**Issue:** No plan for "what if MVP fails?"

**Recommendation:**
```
DECISION POINTS:

After Week 1:
‚ùå If metrics collection doesn't work ‚Üí Stop and reassess
‚úÖ If works ‚Üí Continue

After Week 2:
‚ùå If frontend can't connect to backend ‚Üí Get human help
‚ùå If safety limits don't work ‚Üí Critical issue, fix first
‚úÖ If works ‚Üí Continue

After Week 4:
‚ùå If MVP doesn't demo well ‚Üí Extend timeline
‚úÖ If works ‚Üí Proceed to v1.1 or v1.2

PIVOT OPTIONS:
- Reduce scope further (2 actions instead of 4)
- Change tech stack (Python instead of Go?)
- Switch to simpler project
```

---

### Missing #2: Performance Benchmarks

**Issue:** No specific performance targets for actions

**Recommendation:**
```
PERFORMANCE REQUIREMENTS:

Action Latency:
- API response: <200ms
- Action start: <500ms
- Metric visible: <2s

Action Effectiveness:
- CPU Stress: Must reach 85-95% CPU
- Memory Surge: Must allocate 90-100% of requested memory
- Disk Storm: Must complete all operations
- Traffic Flood: Must generate target requests/sec

If not meeting targets:
- Debug and optimize
- Or adjust targets to realistic values
```

---

### Missing #3: Browser Compatibility

**Issue:** No mention of which browsers to support

**Recommendation:**
```
BROWSER SUPPORT:

MVP (v1.0):
‚úÖ Chrome/Edge latest
‚úÖ Firefox latest
‚ùå Safari (test if works, but not blocker)
‚ùå Mobile browsers (defer to v1.1)
‚ùå IE11 (not supported)

Why?
- Recharts works best in Chrome/Firefox
- Saves testing time
- Can add Safari/mobile later

Action: Add to MVP-PLAN.md
```

---

### Missing #4: Error Messages Strategy

**Issue:** No guidelines for user-facing error messages

**Recommendation:**
```
ERROR MESSAGE STANDARDS:

Bad:
"Error: 500"
"nil pointer dereference"
"HTTP request failed"

Good:
"Could not start CPU stress. Please try again."
"System is busy. Stop other actions first."
"Action timed out. This might mean the system is overloaded."

Include in AI-DEVELOPMENT-RULES.md:
- Rule 28 already exists, but needs examples
- Add to MVP checklist: "All errors have friendly messages"
```

---

## üéØ DOCUMENT REDUNDANCY ISSUES

### Issue: Too Many Overlapping Documents

**Problem:**
```
system-monitor-dashboard-project-plan.md: 1800 lines
MVP-PLAN.md: 645 lines
RELEASE-PLAN.md: 750 lines

Lots of overlap and duplication
```

**Recommendation:**
```
SIMPLIFY DOCUMENTATION STRUCTURE:

Keep:
‚úÖ MASTER-PLAN.md (executive summary, navigation)
‚úÖ MVP-PLAN.md (4-6 week detailed plan)
‚úÖ AI-DEVELOPMENT-RULES.md (rules for AI)
‚úÖ VERSION-ROADMAP.md (version strategy)

Archive (move to /docs/archive/):
üì¶ system-monitor-dashboard-project-plan.md
   - Too detailed
   - Has inconsistencies
   - Overlaps with other docs
   - Keep for reference but don't use as primary

Simplify:
‚úÖ RELEASE-PLAN.md
   - Remove Phase 1 details (already in MVP-PLAN.md)
   - Keep only Phase 1.5 (Showcase) and Phase 2+ (AWS)

Reference as needed:
‚úÖ FEATURE-SUMMARY.md (visual examples)
‚úÖ FUTURE-FEATURES.md (post-MVP ideas)

Result:
- Less confusion
- Single source of truth
- Easier for AI to follow
```

---

## üö® CRITICAL RECOMMENDATIONS

### Recommendation #1: Create Single Source of Truth

**Action Items:**
1. Archive system-monitor-dashboard-project-plan.md
2. Make MVP-PLAN.md the ONLY source for "what to build"
3. Make AI-DEVELOPMENT-RULES.md the ONLY source for "how to build"
4. Update all cross-references

---

### Recommendation #2: Fix All Contradictions

**Action Items:**
1. ‚úÖ WebSocket: NO in MVP (polling only)
2. ‚úÖ Database: NO in MVP (in-memory only)
3. ‚úÖ Scenarios: NO in MVP (v1.3 later)
4. ‚úÖ Timeline: 4-6 weeks realistic
5. ‚úÖ Folder: internal/actions/ (not loadgen)
6. ‚úÖ Coverage: 70% minimum, 75% target for MVP

Update these in ALL documents consistently.

---

### Recommendation #3: Add Missing Test Plan

**Action Items:**
Create TESTING-PLAN.md with:
- Unit test requirements
- Integration test requirements
- Safety test suite (CRITICAL)
- E2E test scenarios
- Performance benchmarks
- Browser compatibility

---

### Recommendation #4: Simplify Phase Numbering

**Action Items:**
RELEASE-PLAN.md should have:
- Phase 1: Local MVP (v1.0)
- Phase 1.5: Showcase (v1.2)
- Phase 2: AWS Dev (v2.0)
- Phase 3: AWS Prod (v2.1)

No sub-phases like 1.1, 1.2, 1.3... confusing with version numbers

---

## ‚úÖ WHAT'S ACTUALLY GOOD

### Strengths of Current Plan:

1. **Safety-First Mindset** ‚úÖ
   - Clear hardware protection limits
   - Emergency shutdown logic
   - Resource cleanup requirements

2. **Incremental Approach** ‚úÖ
   - Local first, cloud later
   - MVP before features
   - Test before deploy

3. **Clear AI Guidelines** ‚úÖ
   - AI-DEVELOPMENT-RULES.md is excellent
   - Checkpoint system is good
   - When to ask for help is clear

4. **Showcase Option** ‚úÖ
   - Phase 1.5 solves "show online" need
   - Good middle ground before AWS
   - Realistic cost estimates

5. **Version Strategy** ‚úÖ
   - Clear v1.0 ‚Üí v1.1 ‚Üí v1.2 progression
   - Decision points at each version
   - Can stop at any milestone

---

## üéØ PRIORITY FIX LIST

### Before Starting Development:

**MUST FIX (Critical):**
1. [ ] Resolve WebSocket contradiction (Use polling for MVP)
2. [ ] Resolve database contradiction (No PostgreSQL in MVP)
3. [ ] Resolve scenarios contradiction (Not in MVP)
4. [ ] Fix folder naming (Use internal/actions/)
5. [ ] Update timeline consistently (4-6 weeks)
6. [ ] Archive system-monitor-dashboard-project-plan.md

**SHOULD FIX (Important):**
7. [ ] Add safety test suite to MVP-PLAN.md
8. [ ] Clarify phase vs version numbering
9. [ ] Update cost estimates with current pricing
10. [ ] Add browser compatibility requirements
11. [ ] Fix showcase deployment timeline (3-5 days not 2-3)

**NICE TO FIX (Improvements):**
12. [ ] Simplify RELEASE-PLAN.md (remove Phase 1 details)
13. [ ] Add performance benchmarks
14. [ ] Add error message examples
15. [ ] Consider optimization suggestions

---

## üéØ RECOMMENDED ACTION PLAN

### Option A: Fix Everything First (RECOMMENDED)
```
1. Spend 2-3 hours fixing all contradictions
2. Create clean, consistent documentation
3. Archive redundant documents
4. THEN start development with AI

Benefits:
- No confusion during development
- AI has clear instructions
- Fewer blockers
- Better outcome

Timeline: +2-3 hours upfront, saves days later
```

### Option B: Quick Fixes, Start Development
```
1. Fix only MUST FIX items (1 hour)
2. Start development
3. Fix SHOULD FIX items as needed

Benefits:
- Faster to start
- Fix issues when they come up

Risks:
- AI might get confused
- May build wrong things
- Need to refactor later

Timeline: +1 hour upfront, might waste days later
```

### Option C: Start Now, Fix When Blocked
```
1. Start development immediately
2. Fix issues when AI asks

Risks:
- High chance of building wrong things
- Wasted effort
- Frustration

NOT RECOMMENDED
```

---

## üìä FINAL ASSESSMENT

### Overall Plan Quality:
**7/10** - Good foundation, but needs cleanup

**Strengths:**
- Safety-conscious ‚úÖ
- Incremental approach ‚úÖ
- Clear AI rules ‚úÖ
- Showcase option ‚úÖ

**Weaknesses:**
- Contradictions between documents ‚ùå
- Redundant documentation ‚ùå
- Some unrealistic timelines ‚ùå
- Missing test strategy ‚ùå

### Risk Assessment:

**If you start without fixes:**
- 60% chance AI builds WebSocket (wrong!)
- 40% chance AI sets up PostgreSQL (wasted time!)
- 70% chance confusion about scenarios
- 50% chance folder structure mismatch

**If you fix first:**
- 90% chance smooth development
- Clear path for AI
- Fewer surprises
- Better outcome

---

## üéØ MY RECOMMENDATION

**DO THIS:**

1. **NOW (30 minutes):**
   - I'll create a clean MVP-PLAN-v2.md fixing all contradictions
   - Archive system-monitor-dashboard-project-plan.md
   - Update AI-DEVELOPMENT-RULES.md with clarifications

2. **THEN (1 hour):**
   - You review MVP-PLAN-v2.md
   - Confirm all decisions (polling vs WebSocket, etc.)
   - Approve to start

3. **AFTER APPROVAL:**
   - AI reads clean documentation
   - AI starts Week 1, Day 1
   - No confusion, smooth development

**BOTTOM LINE:**
The plan is 80% good, but the 20% contradictions will cause problems.
Spending 2 hours fixing now saves days of wasted development.

**Ready for me to create the fixed versions?**
